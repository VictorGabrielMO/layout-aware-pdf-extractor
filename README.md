## üß© Desafio de Extra√ß√£o de Informa√ß√µes de Documentos

Este projeto implementa uma **API de extra√ß√£o de informa√ß√µes de documentos PDF** utilizando **LLM (Large Language Models)**, com foco em **efici√™ncia e reuso de conhecimento** atrav√©s de uma **mem√≥ria de layout persistente** e **cache de resultados**.

---

### üöÄ Vis√£o Geral da Solu√ß√£o

O sistema √© composto por um pipeline modular que processa PDFs e aprende progressivamente o layout dos documentos para reduzir drasticamente o tempo de execu√ß√£o em execu√ß√µes subsequentes.  
A arquitetura foi desenhada para que o modelo de linguagem seja acionado **apenas quando estritamente necess√°rio**, com base em **heur√≠sticas estat√≠sticas** e **armazenamento incremental de padr√µes (regexes)**.

---

### ‚öôÔ∏è Pipeline de Processamento

A pipeline √© composta pelas seguintes etapas principais:

#### 1. **Extra√ß√£o de texto e blocos (PDFParser)**
- O PDF √© processado em duas representa√ß√µes:
  - Texto plano completo (`extract_plain_text`), usado para fingerprint e cache.
  - Blocos textuais estruturados (`extract_text_blocks`), contendo texto e coordenadas normalizadas (px, py).
- Essa granularidade permite an√°lises espaciais precisas no layout.

#### 2. **Pr√©-processamento dos blocos (Preprocessor)**
- Os blocos s√£o limpos e normalizados (remo√ß√£o de ru√≠do, quebras e padr√µes redundantes).
- A sa√≠da √© uma lista de blocos prontos para serem analisados pela heur√≠stica de layout.

#### 3. **Mem√≥ria de Layout (LayoutMemory)**
Essa √© a **camada central de intelig√™ncia** da solu√ß√£o.

- Cada campo de cada tipo de documento √© armazenado com:
  - As **coordenadas m√©dias (px, py)** em que o campo aparece.
  - As **vari√¢ncias acumuladas (M2_px, M2_py)**.
  - Um **regex aprendido** pelo LLM, usado para extra√ß√µes diretas futuras.
- Os valores s√£o atualizados incrementalmente com o **algoritmo de Welford**, o que garante m√©dias e vari√¢ncias corretas sem reprocessar o hist√≥rico.

##### üß† Heur√≠stica de Intervalo de Confian√ßa (CI)
Para cada campo, √© calculado um **intervalo de confian√ßa (IC)** das posi√ß√µes:

![equation](https://latex.codecogs.com/svg.image?IC_{px}=\bar{px}\pm%20z\cdot\frac{\sigma_{px}}{\sqrt{n}})

![equation](https://latex.codecogs.com/svg.image?IC_{py}=\bar{py}\pm%20z\cdot\frac{\sigma_{py}}{\sqrt{n}})


- Se o intervalo √© **estreito (alta confian√ßa)** e o n√∫mero de amostras √© suficiente, assume-se que o campo √© **posicionalmente est√°vel**.
- Assim, √© poss√≠vel identificar diretamente o bloco correspondente **sem consultar o LLM**.

Essa heur√≠stica reduz drasticamente o custo das infer√™ncias:
- Os primeiros documentos de cada tipo demandam chamadas ao LLM (pois √© necess√°rio obter os regexes);
- As execu√ß√µes seguintes reutilizam o conhecimento armazenado, tornando o processo **quase instant√¢neo**.

##### üåê Signific√¢ncia e decis√£o de fallback
O sistema classifica cada campo como:
- `high` ‚Üí coordenadas altamente confi√°veis (dispensa LLM);
- `medium` ‚Üí coordenadas moderadamente est√°veis (uso h√≠brido);
- `low` ‚Üí inst√°vel, depende do LLM.

Observa√ß√£o: Vale salientar que, embora promissora, a abordagem √© heur√≠stica e ainda demanda ajustes e testes emp√≠ricos 
para ajustar, sobretudo, a avalia√ß√£o de hyperpar√¢metros para definir a signific√¢ncia do Intervalo de Confian√ßa. Pretendo ainda explorar isso.

#### 4. **LLM Processor**
- Quando a mem√≥ria de layout n√£o √© suficiente, o pipeline constr√≥i um *prompt contextualizado* e envia ao LLM.
- O modelo retorna:
  - `valor` extra√≠do
  - `regex` usado para encontr√°-lo (Caso uma chamada anterior j√° n√£o houver obtido o regex para o campo analisado)
  - `indice do bloco de origem` usado na atualiza√ß√£o da media e vari√¢ncia do campo

#### 5. **Cache de resultados (document-level cache)**
Para acelerar ainda mais, h√° um **cache persistente para a tupla (texto do documento,label, campo)**:
- √â gerado um *fingerprint SHA256*.
- Se uma requisi√ß√£o id√™ntica j√° foi processado, o resultado √© retornado diretamente sem nova an√°lise.

---

### üìâ Otimiza√ß√µes de Desempenho

| T√©cnica | Descri√ß√£o | Impacto |
|----------|------------|---------|
| **Mem√≥ria de Layout com IC** | Aprende posi√ß√µes m√©dias e varia√ß√£o dos campos. | Reduz consultas ao LLM conforme o uso aumenta. |
| **Armazenamento incremental (Welford)** | Atualiza estat√≠sticas sem reprocessar hist√≥rico. | Mant√©m desempenho est√°vel e preciso. |
| **Cache de documentos (SHA256)** | Retorna resultados instantaneamente para PDFs j√° processados. | Evita recomputa√ß√£o e chamadas √† API. |
| **LRU caching interno** | Minimiza acesso ao banco SQLite. | Diminui lat√™ncia de consultas repetidas. |

üí° **Tend√™ncia natural de desempenho:**  
As primeiras requisi√ß√µes de um tipo de documento ser√£o lentas (depend√™ncia do LLM), mas o sistema convergir√° rapidamente para tempo de execu√ß√£o muito baixo conforme acumula conhecimento.

---

### üß© Exemplo de Fluxo Simplificado

1. Recebe PDF de Nota Fiscal (`label="nota_fiscal"`).  
2. LayoutMemory ainda vazio ‚Üí tudo vai para o LLM.  
3. LLM retorna valores + regex + posi√ß√µes.  
4. Sistema atualiza mem√≥ria com m√©dia, vari√¢ncia e regex.  
5. Eventualmente, ap√≥s a popula√ß√£o do LayoutMemory (Idealmente):  
   - Blocos s√£o casados via IC e regex.  
   - Somente campos n√£o encontrados v√£o ao LLM.  

Resultado: **redu√ß√£o progressiva do custo por documento.**

---

### üß∞ Tecnologias Utilizadas

- **FastAPI** ‚Äî backend e endpoint `/extract`
- **SQLite** ‚Äî armazenamento leve e persistente da mem√≥ria de layout
- **OpenAI API** ‚Äî processamento de linguagem natural e aprendizado de padr√µes
- **Pydantic** ‚Äî valida√ß√£o de entrada e sa√≠da
- **Docker** ‚Äî empacotamento e execu√ß√£o isolada

---

## üöÄ Como Executar a Aplica√ß√£o

### üê≥ Executando com Docker (recomendado)

A maneira mais simples de rodar a aplica√ß√£o √© usando o Docker.  
Basta garantir que voc√™ tenha o Docker instalado e executar os comandos abaixo:

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/VictorGabrielMO/layout-aware-pdf-extractor.git
cd layout-aware-pdf-extractor

# 2. Crie a imagem Docker
docker build -t doc-extraction .

# 3. Rode o container
docker run -p 8000:8000 -e OPENAI_API_KEY=<sua_chave_openai> doc-extraction

```
Ap√≥s a inicializa√ß√£o, basta acessar a **interface gr√°fica** em:  
üëâ [http://localhost:8000](http://localhost:8000)

A interface permite enviar um PDF, informar o r√≥tulo do documento e fornecer o esquema JSON para extra√ß√£o dos campos.

---

## üì° Endpoint de Extra√ß√£o

Al√©m da interface gr√°fica, a API tamb√©m disponibiliza o endpoint:

```
POST /extract
```

### Par√¢metros esperados (multipart/form-data)

| Campo        | Tipo         | Descri√ß√£o |
|---------------|---------------|------------|
| `pdf`         | `file`        | Arquivo PDF a ser processado |
| `label`       | `string`      | Tipo de documento (ex: "nota_fiscal", "contrato") |
| `schema_json` | `string` (JSON) | Estrutura com os campos esperados e suas descri√ß√µes |

### Exemplo de requisi√ß√£o `curl`:
```bash
curl -X POST http://localhost:8000/extract \
  -F "pdf=@exemplo.pdf" \
  -F "label=nota_fiscal" \
  -F 'schema_json={"CNPJ":"N√∫mero do CNPJ da empresa","Data":"Data de emiss√£o"}'
```

### üìä Considera√ß√µes Finais

A principal proposta dessa solu√ß√£o √© transformar o processo de extra√ß√£o de dados via LLM ‚Äî tipicamente caro e lento ‚Äî em um **sistema de aprendizado cont√≠nuo de layout**, com:
- Redu√ß√£o adaptativa de custo;
- Autoaprendizado de padr√µes;
- Independ√™ncia crescente do modelo de linguagem.

Com o uso, o pipeline se comporta como um **extrator cognitivo especializado**, otimizando-se de forma aut√¥noma com base nas intera√ß√µes anteriores.
